diff --git a/config/InvertedPendulum_robust.json b/config/InvertedPendulum_robust.json
index 24ea9ce..60fb0cd 100644
--- a/config/InvertedPendulum_robust.json
+++ b/config/InvertedPendulum_robust.json
@@ -21,6 +21,12 @@
             "action_eps_scheduler": {
                 "end": 0.2
             }
+        },
+        "OARL": false,
+        "OARL_params": {
+            "rate": 0.85,
+            "model_name": "Pendululm/Pendulum_State_Dense.keras",
+            "lstm": false
         }
     },
     "data_config": {
diff --git a/config/InvertedPendulum_vanilla.json b/config/InvertedPendulum_vanilla.json
index 7eb7285..00606f6 100644
--- a/config/InvertedPendulum_vanilla.json
+++ b/config/InvertedPendulum_vanilla.json
@@ -12,6 +12,7 @@
         }
     },
     "test_config": {
+
         "attack_params": {
             "eps": 0.3,
             "sarsa_action_ratio": 0.9999
diff --git a/deep_rl/agent/BaseAgent.py b/deep_rl/agent/BaseAgent.py
index b2a8e21..f6ada3a 100644
--- a/deep_rl/agent/BaseAgent.py
+++ b/deep_rl/agent/BaseAgent.py
@@ -11,13 +11,16 @@ from ..utils import *
 import torch.multiprocessing as mp
 from collections import deque, OrderedDict
 from skimage.io import imsave
+from tensorflow import keras
 
 
 class BaseAgent:
-    def __init__(self, config):
+    def __init__(self, config, OARL=False):
         self.config = config
         self.logger = get_logger(tag=config.tag, log_level=config.log_level, models_path=config.models_path)
         self.task_ind = 0
+        self.oarl_model = None
+        self.OARL = OARL
 
     def close(self):
         close_obj(self.task)
@@ -37,6 +40,14 @@ class BaseAgent:
         else:
             self.logger.info("Not intializing normalizer because {} does not exist.".format(normalizer_file))
 
+        if self.OARL and self.config.OARL:
+            if os.path.exists('OARL_prediction/{}'.format(self.config.OARL_params['model_name'])):
+                self.oarl_model = keras.models.load_model('OARL_prediction/{}'.format(self.config.OARL_params['model_name']))
+            else:
+                print("OARL path: {}\n does not exist".format('OARL_prediction/{}'.format(self.config.OARL_params['model_name'])))
+                exit(1)
+
+
     def eval_step(self, state):
         raise NotImplementedError
 
@@ -44,7 +55,12 @@ class BaseAgent:
         if show is None:
             show = self.config.show_game
         env = self.config.eval_env
+
+
         state = env.reset()
+
+        state_shape = np.asarray(state).shape
+
         states = []
         actions = []
         certify_losses_l1 = []
@@ -54,6 +70,7 @@ class BaseAgent:
         steps = 0
         frame_steps = 0
         save_dir = os.path.join(self.config.models_path, "frames", "{:03d}".format(episode_number))
+        transitions = []
         if self.config.save_frame:
             os.makedirs(save_dir, exist_ok=True)
         while True:
@@ -65,7 +82,36 @@ class BaseAgent:
                 certify_losses_range.append(certify_loss_range)
             else:
                 action = self.eval_step(state)
-            state, reward, done, info = env.step(action)
+            action_shape = np.asarray(action).shape
+            next_state, reward, done, info = env.step(action)
+
+            if self.OARL and self.config.OARL and np.random.random() < self.config.OARL_params['rate']:
+                if not self.config.OARL_params['lstm']:
+                    state = np.asarray(state)
+                    action = np.asarray(action)
+                    oarl_input = np.concatenate([state, action], axis=1)
+                    next_state = self.oarl_model.predict_on_batch(oarl_input)
+                    next_state = np.reshape(next_state, state_shape)
+                    predicted = True
+                else:
+                    if steps > 2:
+                        oarl_states = states[-3:]
+                        oarl_actions = actions[-3:]
+                        oarl_states = np.reshape(oarl_states, [1, 3, state_shape[1]])
+                        oarl_actions = np.reshape(oarl_actions, [1, 3, action_shape[1]])
+                        oarl_input = np.concatenate([oarl_states, oarl_actions], axis=2)
+                        next_state = self.oarl_model.predict_on_batch(oarl_input)
+                        predicted = True
+
+                    else:
+                        predicted = False
+            else:
+                predicted = False
+
+
+            #print(np.asarray(done).shape)
+            transitions.append((state[0], action[0], next_state[0], done, predicted))
+            state = next_state
             states.append(state[0])
             actions.append(action[0])
             for e in env.env.envs:
@@ -82,7 +128,7 @@ class BaseAgent:
                 break
             steps += 1
         if return_states:
-            return ret, states, actions, certify_losses_l1, certify_losses_l2, certify_losses_linf, certify_losses_range
+            return ret, states, actions, certify_losses_l1, certify_losses_l2, certify_losses_linf, certify_losses_range, transitions
         return ret
 
     def eval_episodes(self):
diff --git a/defaults.json b/defaults.json
index 9da1796..52cad90 100644
--- a/defaults.json
+++ b/defaults.json
@@ -62,7 +62,9 @@
         }
     },
     "test_config": {
-        "eval_episodes": 50,
+        "save_transition_path": "",
+        "OARL": false,
+        "eval_episodes": 100,
         "show_game": false,
         "save_frame": false,
         "certify": false,
@@ -103,6 +105,11 @@
             "enabled": false,
             "use_full_backward": false,
             "eps": "auto"
+        },
+        "OARL_params": {
+            "rate": 0.85,
+            "model_name": "Ant/Ant_State_Dense.keras",
+            "lstm": false
         }
     },
     "data_config": {
diff --git a/eval_ddpg.py b/eval_ddpg.py
index 626f938..20a371c 100644
--- a/eval_ddpg.py
+++ b/eval_ddpg.py
@@ -43,6 +43,12 @@ def ddpg_eval(config):
         agent.load_sarsa(best_model)
     else:
         agent.load(best_model)
+
+    if config.save_transition_path is not None:
+        print("will save transition")
+    else:
+        print("will not save transition")
+
     episodic_returns = []
     all_states = []
     all_actions = []
@@ -51,8 +57,15 @@ def ddpg_eval(config):
     certify_losses_linf = []
     certify_losses_range = []
     for ep in range(agent.config.eval_episodes):
-        total_rewards, states, actions, certify_loss_l1, certify_loss_l2, certify_loss_linf, certify_loss_range = agent.eval_episode(show=agent.config.show_game, return_states=True,
+        total_rewards, states, actions, certify_loss_l1, certify_loss_l2, certify_loss_linf, certify_loss_range, tran = agent.eval_episode(show=agent.config.show_game, return_states=True,
                 certify_eps=agent.config.certify_params["eps"] if agent.config.certify_params["enabled"] else 0.0, episode_number=ep)
+
+        if ep == 0:
+            transitions = np.asarray(tran)
+        else:
+            transitions = np.concatenate([transitions, np.asarray(tran)], axis=0)
+            print(transitions.shape)
+
         if agent.config.certify_params["enabled"]:
             agent.logger.info('epoch %d reward %f steps %d certified_loss l1=%.4f l2=%.4f linf=%.4f range=%.4f', ep, total_rewards, len(states),
                     (np.mean(certify_loss_l1)), (np.mean(certify_loss_l2)), (np.mean(certify_loss_linf)), (np.mean(certify_loss_range)))
@@ -86,9 +99,26 @@ def ddpg_eval(config):
         agent.logger.info('Average certify loss range: %f, max certify loss: %f, std: %f', np.mean(certify_losses_range), np.max(certify_losses_range), np.std(certify_losses_range))
     agent.logger.info('[END-END-END] Finishing evaluation')
 
+    if config.save_transition_path is not None:
+        outfile = config.save_transition_path
+        outfile = outfile + "_transition.npy"
+
+        transitions = np.asarray(transitions)
+        np.save(outfile, transitions)
+
+        """
+        Pendulum: State (4,) Action (1,) (action is cts -3 to 3)
+        Ant: State (111,) Action (8,) (action is cts -1 to 1)
+        Hopper: State (11,) Action (3,) (action is cts -1 to 1)
+        """
+        print(transitions.shape)
+        print('State shape: {}'.format(transitions[0, 0].shape))
+        print('Action shape: {}'.format(transitions[0, 1].shape))
+
+
 
 # config_dict is our configurations set in JSON file
-def ddpg_eval_setup(config_dict,suffix=""):
+def ddpg_eval_setup(config_dict,suffix="", eval=True):
     # Read config file and translate relevant fields
     ddpg_config = {}
     ddpg_config['game'] = config_dict['env_id']
@@ -102,9 +132,19 @@ def ddpg_eval_setup(config_dict,suffix=""):
 
     config.task_fn = lambda: Task(config.game)
     config.eval_env = config.task_fn()
+    print(config.eval_env)
     config.eval_episodes = eval_config['eval_episodes']
     config.tag += "_eval"
 
+    config.OARL = eval_config['OARL']
+    if config.OARL:
+        config.OARL_params = eval_config['OARL_params']
+
+
+    if config_dict["test_config"]["save_transition_path"] is not "":
+        config.save_transition_path = config_dict["test_config"]["save_transition_path"]
+    else:
+        config.save_transition_path = None
     # newly added properties
     config.models_path = config_dict["models_path"]
     config.show_game = eval_config['show_game']
@@ -128,8 +168,10 @@ def ddpg_eval_setup(config_dict,suffix=""):
         print('robust_params[advtrain_scheduler][end] is setting to', config.robust_params['advtrain_scheduler']['end'])
     config.certify_params = eval_config['certify_params']
     config.attack_params = eval_config["attack_params"]
+    #This is perturbation per step
+    print("ATTACK EPSILON IS SET TO", config.attack_params['eps'])
     if config.attack_params['alpha'] == "auto":
-        config.attack_params['alpha'] = config.attack_params['eps'] / config.attack_params['iteration']
+        config.attack_params['alpha'] = config.attack_params['eps'] / config.attack_params['iteration'] #Adjust config.attack_params['eps'] to adjust attack eps
         print('config.attack_params[alpha] is setting to', config.attack_params['alpha'])
     if config.certify_params['eps'] == "auto":
         config.certify_params['eps'] = config.attack_params['eps']
@@ -158,11 +200,13 @@ def ddpg_eval_setup(config_dict,suffix=""):
 
     sys.stdout.flush()
     sys.stderr.flush()
-    ddpg_eval(config)
 
+    ddpg_eval(config)
 
+USE_CUDA = torch.cuda.is_available()
 def main(args):
     config = load_config(args)
+    print(config["test_config"]["save_transition_path"])
     # game_list = ['HalfCheetah-v2', 'Walker2d-v2', 'Hopper-v2', 'Swimmer-v2', 'Reacher-v2', 'InvertedPendulum-v2', 'Ant-v2', 'Humanoid-v2']
     config['models_path'] = os.path.join(args.path_prefix, config['models_path'].format(env_id=config['env_id']))
     mkdir(config['models_path'])
@@ -170,11 +214,16 @@ def main(args):
     mkdir(os.path.join(config['models_path'], 'log'))
     mkdir(os.path.join(config['models_path'], 'tf_log'))
     mkdir(os.path.join(config['models_path'], 'runtime'))
-    if 'CUDA_VISIBLE_DEVICES' in os.environ and os.environ['CUDA_VISIBLE_DEVICES'] == '-1':
+
+
+
+    print("Use_cuda: {}".format(USE_CUDA))
+    if not USE_CUDA:
+    #if 'CUDA_VISIBLE_DEVICES' in os.environ and os.environ['CUDA_VISIBLE_DEVICES'] == '-1':
         # Use CPU.
-        del os.environ['CUDA_VISIBLE_DEVICES'] # Avoid HIP error.
-        if 'HIP_VISIBLE_DEVICES' in os.environ:
-            del os.environ['HIP_VISIBLE_DEVICES'] # Avoid HIP error.
+        #del os.environ['CUDA_VISIBLE_DEVICES'] # Avoid HIP error.
+        #if 'HIP_VISIBLE_DEVICES' in os.environ:
+            #del os.environ['HIP_VISIBLE_DEVICES'] # Avoid HIP error.
         print('Using CPU.')
         select_device(-1)
     else:
@@ -182,6 +231,7 @@ def main(args):
         select_device(0)
     suffix = ""
     if config['test_config']['attack_params']['enabled'] == True and config['test_config']['attack_params']['type'].startswith('sarsa'):
+
         # if need to train a model or not
         scheduler_opts = config['test_config']['sarsa_params']['action_eps_scheduler']
         suffix = "_{start}_{end}_{steps}_{start_step}".format(**scheduler_opts) + "_{}".format(config['test_config']['sarsa_params']['sarsa_reg'])
@@ -191,7 +241,7 @@ def main(args):
             # need to train a model 
             print(f"Existing value function {sarsa_model_filename} does not exist. Will train a new one.")
             from train_sarsa import ddpg_continuous
-
+            #ddpg_eval_setup(config)
             ddpg_continuous(config)
     global RobustDDPGAgent, RobustDeterministicActorCriticNet
     # evaluation 
